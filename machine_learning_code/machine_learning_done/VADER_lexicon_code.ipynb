{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ” Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import string\n",
    "import editdistance\n",
    "from textstat import flesch_reading_ease\n",
    "import nltk\n",
    "import MoreThanSentiments as mts\n",
    "from nltk.util import pairwise \n",
    "from vaderSentiment.vaderSentiment import NEGATE, BOOSTER_DICT\n",
    "import math\n",
    "import re\n",
    "from itertools import product\n",
    "import nltk.data\n",
    "from vaderSentiment.vaderSentiment import NEGATE, BOOSTER_DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the working directory\n",
    "os.chdir('C:/Users/jesse/OneDrive/Documenten/Thesis/amazon_code/dataframes_done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the reviews from the JSON file\n",
    "with open('output.json', 'r') as f:\n",
    "    reviews = json.load(f)\n",
    "\n",
    "# DataFrame\n",
    "reviews_df = pd.DataFrame(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantity\n",
    "result_quantity = []\n",
    "\n",
    "for review in reviews:\n",
    "    text = review['Review_Text']\n",
    "    # Number of words\n",
    "    words = text.split()\n",
    "    num_words = len(words)\n",
    "    # Number of sentences\n",
    "    sentences = text.split('.')\n",
    "    num_sentences = len(sentences)\n",
    "    # Number of caps\n",
    "    num_caps = sum(1 for c in text if c.isupper())\n",
    "    # Number of punctuation\n",
    "    num_punctuation = sum(text.count(p) for p in string.punctuation)\n",
    "     # Part of speech\n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "    noun_count = len([word for word, pos in pos_tags if pos in ['NN', 'NNS', 'NNP', 'NNPS']])\n",
    "    verb_count = len([word for word, pos in pos_tags if pos in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']])\n",
    "    adj_count = len([word for word, pos in pos_tags if pos in ['JJ', 'JJR', 'JJS']])\n",
    "    adv_count = len([word for word, pos in pos_tags if pos in ['RB', 'RBR', 'RBS']])\n",
    "\n",
    "\n",
    "    # linguistic features results\n",
    "    result_quantity.append({\n",
    "        'Number_of_words': num_words,\n",
    "        'Number_of_sentences': num_sentences,\n",
    "        'Number_of_caps': num_caps,\n",
    "        'Number_of_punctuation': num_punctuation,\n",
    "        'Number_of_nouns': noun_count,\n",
    "        'Number_of_verbs': verb_count,\n",
    "        'Number_of_adjectives': adj_count,\n",
    "        'Number_of_adverbs': adv_count\n",
    "    })\n",
    "\n",
    "# Create a result dataframe\n",
    "df_qua_VADER = pd.DataFrame(result_quantity)\n",
    "\n",
    "# Dataframe to Json file\n",
    "df_qua_VADER.to_json('df_qua_VADER.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22284/1011619988.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mavg_sentence_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_sentence_length\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnum_sentences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;31m# Redundance score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0mredundancy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_redundancy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m     \u001b[1;31m# Readability score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mreadability_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflesch_reading_ease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22284/1011619988.py\u001b[0m in \u001b[0;36mcalculate_redundancy\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# Calculate the sum of Levenshtein distances between all pairs of words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mtotal_distance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meditdistance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw1\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m# Calculate the average Levenshtein distance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22284/1011619988.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# Calculate the sum of Levenshtein distances between all pairs of words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mtotal_distance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meditdistance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw1\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m# Calculate the average Levenshtein distance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Complexity\n",
    "\n",
    "# Redundancy function\n",
    "def calculate_redundancy(text):\n",
    "    words = text.split()\n",
    "    \n",
    "    # Sum of Levenshtein distances between all pairs of words\n",
    "    total_distance = sum(editdistance.eval(w1, w2) for i, w1 in enumerate(words) for j, w2 in enumerate(words) if i < j)\n",
    "    \n",
    "    # Average Levenshtein distance\n",
    "    n = len(words)\n",
    "    if n > 1:\n",
    "        average_distance = total_distance / (n * (n - 1) / 2)\n",
    "    else:\n",
    "        average_distance = 0\n",
    "    \n",
    "    # Return the redundancy\n",
    "    return 1 - average_distance / len(max(words, key=len))\n",
    "\n",
    "results_complexity = []\n",
    "\n",
    "for review in reviews:\n",
    "    text = review['Review_Text']\n",
    "    # Number of words\n",
    "    words = text.split()\n",
    "    num_words = len(words)\n",
    "    # Number of sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    num_sentences = len(sentences)\n",
    "    # Average word length\n",
    "    total_word_length = sum(len(word) for word in words)\n",
    "    avg_word_length = total_word_length / num_words\n",
    "    # Average sentence length\n",
    "    total_sentence_length = sum(len(sent) for sent in sentences)\n",
    "    avg_sentence_length = total_sentence_length / num_sentences\n",
    "    # Redundance score\n",
    "    redundancy = calculate_redundancy(text)\n",
    "    # Readability score\n",
    "    readability_score = flesch_reading_ease(text)\n",
    "\n",
    "    # linguistic features results\n",
    "    results_complexity.append({\n",
    "        'Average_word_length': avg_word_length,\n",
    "        'Average_sentence_length': avg_sentence_length,\n",
    "        'Redundancy_score': redundancy,\n",
    "        'Readability_score': readability_score,\n",
    "})\n",
    "\n",
    "# Result dataframe\n",
    "df_com_VADER = pd.DataFrame(results_complexity)\n",
    "\n",
    "# Dataframe to Json file\n",
    "df_com_VADER.to_json('df_com_VADER.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lexical_diversity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.851852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.584615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.764706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.545455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Lexical_diversity\n",
       "0           0.851852\n",
       "1           0.727273\n",
       "2           0.584615\n",
       "3           0.764706\n",
       "4           0.545455"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Diversity\n",
    "results_diversity = []\n",
    "\n",
    "for review in reviews:\n",
    "    text = review['lemmatized']\n",
    "    words = text\n",
    "    num_words = len(words)\n",
    "    # Lexical diversity\n",
    "    unique_words = set(words)\n",
    "    lexical_diversity = len(unique_words) / num_words\n",
    "\n",
    "    # Linguistic features results\n",
    "    results_diversity.append({\n",
    "        'Lexical_diversity': lexical_diversity\n",
    "    })\n",
    "\n",
    "\n",
    "# Results dataframe\n",
    "df_div_VADER = pd.DataFrame(results_diversity)\n",
    "\n",
    "# Dataframe to Json file\n",
    "df_div_VADER.to_json('df_div_VADER.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>positive_score</th>\n",
       "      <th>negative_score</th>\n",
       "      <th>neutral_score</th>\n",
       "      <th>positive_words</th>\n",
       "      <th>negative_words</th>\n",
       "      <th>neutral_words</th>\n",
       "      <th>polarity_shifters</th>\n",
       "      <th>intensity_modifiers</th>\n",
       "      <th>negations</th>\n",
       "      <th>emoticons</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.7269</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.783</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.8019</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.803</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.7557</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.782</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.6588</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.901</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.7964</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.884</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment_score  positive_score  negative_score  neutral_score  \\\n",
       "0           0.7269           0.217           0.000          0.783   \n",
       "1           0.8019           0.137           0.060          0.803   \n",
       "2           0.7557           0.165           0.053          0.782   \n",
       "3           0.6588           0.099           0.000          0.901   \n",
       "4           0.7964           0.116           0.000          0.884   \n",
       "\n",
       "   positive_words  negative_words  neutral_words  polarity_shifters  \\\n",
       "0             2.0             0.0           22.0                0.0   \n",
       "1             2.0             3.0           65.0                1.0   \n",
       "2             6.0             0.0           45.0                1.0   \n",
       "3             1.0             0.0           40.0                0.0   \n",
       "4             3.0             0.0           62.0                0.0   \n",
       "\n",
       "   intensity_modifiers  negations  emoticons  \n",
       "0                  1.0        0.0        0.0  \n",
       "1                  1.0        2.0        0.0  \n",
       "2                  3.0        3.0        0.0  \n",
       "3                  3.0        1.0        0.0  \n",
       "4                  1.0        0.0        0.0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Emotion\n",
    "\n",
    "# VADER sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Sentiment score function\n",
    "def get_sentiment_scores(text):\n",
    "    scores = sia.polarity_scores(text)\n",
    "    return scores['compound'], scores['pos'], scores['neg'], scores['neu']\n",
    "\n",
    "# Polarity function\n",
    "def get_polarity_categories(text):\n",
    "    words = text.split()\n",
    "    polarities = {'pos': 0, 'neg': 0, 'neu': 0}\n",
    "    for word in words:\n",
    "        scores = sia.polarity_scores(word)\n",
    "        for key in polarities.keys():\n",
    "            if scores[key] > 0:\n",
    "                polarities[key] += 1\n",
    "    return polarities\n",
    "\n",
    "# Polarity shifters function\n",
    "def count_polarity_shifters(text):\n",
    "    shifters = ['but', 'however', 'although', 'yet', 'nevertheless']\n",
    "    count = 0\n",
    "    for word in text:\n",
    "        if word.lower() in shifters:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "# Intensity modifiers function\n",
    "intensity_modifier_words = BOOSTER_DICT\n",
    "def count_intensity_modifiers(text):\n",
    "    modifiers = intensity_modifier_words\n",
    "    count = 0\n",
    "    for word in text:\n",
    "        if word.lower() in modifiers:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "# Negations function\n",
    "negation_words = NEGATE\n",
    "def count_negations(text):\n",
    "    negations = negation_words \n",
    "    count = 0\n",
    "    for word in text:\n",
    "        if word.lower() in negations:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def count_emoticons(text):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    emoticons = re.findall(r'(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    scores = sid.polarity_scores(text)\n",
    "    num_emoticons = len(emoticons)\n",
    "    return num_emoticons\n",
    "\n",
    "# Empty dataframe to store results\n",
    "columns = ['sentiment_score', 'positive_score', 'negative_score', 'neutral_score',            \n",
    "           'positive_words', 'negative_words', 'neutral_words',           \n",
    "           'polarity_shifters', 'intensity_modifiers', 'negations', 'emoticons']\n",
    "df_emo_VADER = pd.DataFrame(columns=columns)\n",
    "\n",
    "# Loop over reviews to extract features\n",
    "for index, row in reviews_df.iterrows():\n",
    "    text = row['lemmatized']\n",
    "    text2 = row['Review_Text']\n",
    "    sentiment_score, positive_score, negative_score, neutral_score = get_sentiment_scores(text2)\n",
    "    polarities = get_polarity_categories(text2)\n",
    "    polarity_shifters = count_polarity_shifters(text)\n",
    "    intensity_modifiers = count_intensity_modifiers(text)\n",
    "    negations = count_negations(text)\n",
    "    emoticons = count_emoticons(text2)\n",
    "    row_results = [sentiment_score, positive_score, negative_score, neutral_score, \n",
    "                   polarities['pos'], polarities['neg'], polarities['neu'],\n",
    "                   polarity_shifters, intensity_modifiers, negations, emoticons]\n",
    "    df_emo_VADER.loc[index] = row_results\n",
    "\n",
    "# write the dataframe to a Json file\n",
    "df_emo_VADER.to_json('df_emo_VADER.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the working directory\n",
    "os.chdir('C:/Users/jesse/OneDrive/Documenten/Thesis/amazon_code/dataframes')\n",
    "\n",
    "# Load dataframes\n",
    "df_qua_VADER = pd.read_json('df_qua_VADER.json')\n",
    "df_com_VADER = pd.read_json('df_com_VADER.json')\n",
    "df_div_VADER = pd.read_json('df_div_VADER.json')\n",
    "df_emo_VADER = pd.read_json('df_emo_VADER.json')\n",
    "\n",
    "# Concatenate dataframes\n",
    "VADER_df = pd.concat([reviews_df[['Label']], df_qua_VADER, df_com_VADER, df_div_VADER, df_emo_VADER], axis=1)\n",
    "\n",
    "# Dataframe to Json file\n",
    "VADER_df.to_json('VADER_df.json', orient='records')\n",
    "\n",
    "VADER_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label                    fake          real\n",
      "DOC_ID    count  10500.000000  10500.000000\n",
      "          mean    5250.500000  15750.500000\n",
      "          std     3031.233247   3031.233247\n",
      "          min        1.000000  10501.000000\n",
      "          25%     2625.750000  13125.750000\n",
      "...                       ...           ...\n",
      "emoticons min        0.000000      0.000000\n",
      "          25%        0.000000      0.000000\n",
      "          50%        0.000000      0.000000\n",
      "          75%        0.000000      0.000000\n",
      "          max        4.000000      6.000000\n",
      "\n",
      "[208 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Rename the labels\n",
    "VADER_df['Label'] = VADER_df['Label'].replace({'__label1__': 'fake', '__label2__': 'real'})\n",
    "\n",
    "# Group the data by label and calculate statistics\n",
    "statistics_VADER = VADER_df.groupby('Label').describe().transpose()\n",
    "\n",
    "# Print statistics\n",
    "print(statistics_VADER)\n",
    "\n",
    "statistics_VADER.to_excel('statistics_VADER.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
